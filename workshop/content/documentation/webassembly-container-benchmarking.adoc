:sectnums:
:sectnumlevels: 3
:markup-in-source: verbatim,attributes,quotes
:imagesdir: ./_images/cockpit-rhel90
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:ssh_username: <Provided-By-Instructor>
:ssh_password: <Provided-By-Instructor>
:targethost_fqdn: <Provided-By-Instructor>
:subdomain: example.com
:format_cmd_exec: source,options="nowrap",subs="{markup-in-source}",role="copy"
:format_cmd_output: bash,options="nowrap",subs="{markup-in-source}"
ifeval::["%cloud_provider%" == "ec2"]
:ssh_password: %ssh_password%
:ssh_username: %ssh_username%
:targethost_fqdn: %targethost%
:subdomain: %subdomain_internal%
:format_cmd_exec: source,options="nowrap",subs="{markup-in-source}",role="execute"
endif::[]



:toc:
:toclevels: 1

= Bonus Lab - Benchmarking native WebAssembly against Containerized WebAssembly

== Overview

This is more a fun lab than a scientific one and to make an accurate determination would take a more rigorous approach. However it is an interesting question, particularly given the focus on WebAssembly with particular workloads such as Serverless and FaaS (Function as a Service).

One of the current focus areas of WebAssembly is FaaS and how `.wasm` has tremendous potential where fast startup is a key attribute. Of course this lab completely ignores how either raw `.wasm` or wasm containers would be scheduled.

Unfortunately our existing container `http_server` runs as a service so let's build a new image with a shorter, more deterministic, runtime.

== Part 1: Building a new image


. Change directory via `cd` into a Rust project `simple_benchmark_test`
+
[{format_cmd_output}]
----
$ cd ~/wasm-oci-source-code-examples/simple_benchmark_test
----
+

NOTE: You can easily inspect the source with `vim src/main.rs` if you want to examine what pair programming with ChatGPT4 looks like. Basically we are simply going to do some simple I/O and calculate some prime numbers to consume a bit more system time than a simple "hello World".

. Build your .wasm artifact with `cargo`
+

[{format_cmd_output}]
----
$ cargo build --target wasm32-wasi --release
----
+

.Sample Output
[source,textinfo]
----
    Finished release [optimized] target(s) in 0.02s
----

. Validate your .wasm file runs with `wasmedge` and use `time` for a very crude measure
+

[{format_cmd_output}]
----
$ time wasmedge ./target/wasm32-wasi/release/simple_benchmark_test.wasm
----
+

.Sample Output
[source,textinfo]
----
Hello, world!
largest prime below 38765432 final value : Some(38765393)

real    0m11.913s
user    0m10.835s
sys     0m0.008s
----

. Build a WebAssembly Container using the supplied Containerfile
+

[{format_cmd_output}]
----
$ time wasmedge ./target/wasm32-wasi/release/simple_benchmark_test.wasm
buildah build --platform wasm/wasi --annotation "run.oci.handler=wasmedge" -t quay.io/tonykay/simple_benchmark_test:0.1.0 .
----
+
NOTE: If you are not planning on pushing the image then you can omit the `<REGISTRY>/<REPO>` in your container name.
+
.Sample Output
[source,textinfo]
----
STEP 1/3: FROM scratch
STEP 2/3: COPY ./target/wasm32-wasi/release/simple_benchmark_test.wasm /
STEP 3/3: CMD ["/simple_benchmark_test.wasm"]
COMMIT quay.io/tonykay/simple_benchmark_test:0.1.0
Getting image source signatures
Copying blob 8619044b0834 skipped: already exists
Copying config f0bdb91bd2 done
Writing manifest to image destination
Storing signatures
--> f0bdb91bd24f
Successfully tagged quay.io/tonykay/simple_benchmark_test:0.1.0
----

. Run you new image, again with `time`
+

[{format_cmd_output}]
----
$ time podman run --rm --name benchmark simple_benchmark_test:0.1.0
----
+
.Sample Output
[source,textinfo]
----
Hello, world!
largest prime below 38765432 final value : Some(38765393)

real    0m12.384s
user    0m0.085s
sys     0m0.046s
----

You will probably see that the Containerized version did in fact take _"a bit longer to run"_ but a couple of runs with time like this is not particularly meaningful. For example `podman` may have already been in memory` whilst `wasmedge` had to be loaded from disk etc., or of course the opposite.

== Part 2: Basic Benchmarks with `hyperfine`

In this part we are going to make our benchmark a bit more meaningful by using `hyperfine` to do:

* warmups e.g. reduce cold start paging issues etc as files are read of disks
* average multiple runs

As stated this is a bonus fun lab and PRs and improvements to the code are gratefully accepted.

. First use `hyperfine` to run the native `wasmedge` version.
+

[{format_cmd_output}]
----
$ hyperfine --shell=none  --warmup 3 --runs 5 "wasmedge ./target/wasm32-wasi/release/simple_benchmark_test.wasm"
----
+

.Sample Output
[source,textinfo]
----
Benchmark 1: wasmedge ./target/wasm32-wasi/release/simple_benchmark_test.wasm
  Time (mean ± σ):     12.022 s ±  0.051 s    [User: 10.885 s, System: 0.011 s]
  Range (min … max):   11.965 s … 12.098 s    5 runs
----

. Repeat the prior test, this time via `podman`
+

[{format_cmd_output}]
----
$ hyperfine --shell=none  --warmup 3 --runs 5 "podman run --rm --name benchmark simple_benchmark_test:0.1.0"
----
+

.Sample Output
[source,textinfo]
----
Benchmark 1: podman run --rm --name benchmark simple_benchmark_test:0.1.0
  Time (mean ± σ):     12.314 s ±  0.075 s    [User: 0.083 s, System: 0.046 s]
  Range (min … max):   12.235 s … 12.434 s    5 runs
----

Certainly on _my machine_ which like yours is most likely a `t3a.medium` running on AWS podman seems only very slightly slower despite having to fork/exec its way to `wasmedge` via `conmon` and `crun`.

= Summary

So hopefully that was a fun and interesting exercise as we saw wasmedge bake off against podman running the containerized version of the same payload.

But as we discussed earlier `podman` is a Container Engine and *not* a Container Runtime, what if we could go straight to `crun`? In our next, also Bonus, lab we'll get `crun` running our OCI image directly which is an interesting exercise in its own right and then performing that simple benchmark.

